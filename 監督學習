#Regression model 回歸模型
#Curve Fitting 曲線擬合
#Single-input Single-output，簡稱SISO 單輸入、單輸出


import torch
import torch.nn.functional as F                                                 #激勵函數
import matplotlib.pyplot as plt                        

input = torch.linspace(-1, 1, 100)                                              #線性間距向量, 一維且均勻分布
x = torch.unsqueeze(input, dim=1)                                               #擴充維度
#print(x.shape)                                                                 #shape=(100, 1)
y = x.pow(2) + 0.2 * torch.rand(x.size())                                       #增加雜訊; torch.rand()均勻分布   

# plt.scatter(x.data.numpy(), y.data.numpy())
# plt.show()

class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(n_feature, n_hidden)                     #hidden layer
        self.predict = torch.nn.Linear(n_hidden, n_output)                     #output layer

    def forward(self, x):
        x = F.relu(self.hidden(x))                                             #激勵函數 for hidden layer
        x = self.predict(x)                                                    #線性輸出
        return x
        
net = Net(n_feature=1, n_hidden=20, n_output=1)
LR = 0.01
BATCH_SIZE = 32
EPOCH = 12

optimizer = torch.optim.SGD(net.parameters(), lr=0.2)                               #隨機梯度下降
# optimizer =  torch.optim.SGD(net.parameters(), lr=LR, momentum=0.8)               #Momentum
# optimizer = torch.optim.RMSprop(net.parameters(), lr=LR, alpha=0.9)               #RMSprop
# optimizer = torch.optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.99))          #Adam
loss_func = torch.nn.MSELoss()                                                 #損失函數, 預測值和真實值的公式（均方差）

plt.ion()                                                                      #啟用交互模式

for t in range(200):
    prediction = net(x)                                                        #輸入 x 並根據 x 進行預測
    loss = loss_func(prediction, y)                                            # (output, target), 計算二者誤差

    optimizer.zero_grad()                                                      #將所有參數的梯度緩衝區（buffer）歸零
    loss.backward()                                                            #反向傳播
    optimizer.step()                                                           #更新權重

    if t % 5 == 0:
        plt.cla()                                                              #清除坐標軸
        plt.scatter(x.data.numpy(), y.data.numpy())                            #散點圖
        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)          #標記x, y
        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})
        plt.pause(0.1)
        
plt.ioff()
plt.show()        


#分類模型

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt

n_data = torch.ones(100, 2)
x0 = torch.normal(2*n_data, 1)                                          #torch.normal(mean, std)
y0 = torch.zeros(100)
x1 = torch.normal(-2*n_data, 1)
y1 = torch.ones(100)
x = torch.cat((x0, x1), 0).type(torch.FloatTensor)                      #合併數據
y = torch.cat((y0, y1), ).type(torch.LongTensor)

# plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=y.data.numpy(), s=100)
# plt.show()

class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.out = torch.nn.Linear(n_hidden, n_output)

    def forward(self, x):
        x = F.relu(self.hidden(x))
        x = self.out(x)
        return x

net = Net(n_feature=2, n_hidden=10, n_output=2)
#取代上面的class
# net = torch.nn.Sequential(                                    
#     torch.nn.Linear(2, 10),
#     torch.nn.ReLU(),
#     torch.nn.Linear(10, 2)
# )

print(net)

optimizer = torch.optim.SGD(net.parameters(), lr=0.02)
loss_func = torch.nn.CrossEntropyLoss()                             #交叉熵(cross-entropy)

plt.ion()

for t in range(100):
    out = net(x)
    loss = loss_func(out, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if t % 2 == 0:
        plt.cla()
        prediction = torch.max(out, 1)[1]
        pred_y = prediction.data.numpy()
        target_y = y.data.numpy()
        plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100)
        accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)                      #[(TP)+(TN)]/全部資料總數
        plt.text(1.5, -4, 'Accuracy=%.2f' % accuracy, fontdict={'size': 20, 'color':  'red'})
        plt.pause(0.1)

plt.ioff()
plt.show()


#Batch training
#避免記憶體不足, 提高處理速度, 提高learning rate(增加batch size)

import torch
import torch.utils.data as Data

torch.manual_seed(1)                                               #設置隨機化初始的種子

BATCH_SIZE = 5

x = torch.linspace(1, 30, 30)
y = torch.linspace(30, 1, 30)

torch_dataset = Data.TensorDataset(x, y)                            #Dataset wrapping tensors
loader = Data.DataLoader(
    dataset=torch_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,                                                   # reshuffled(重新洗牌) at every epoch
    num_workers=2,                                                  # subprocesses for data loading
)

if __name__ == '__main__':
    for epoch in range(3):                                          # train entire dataset 3 times
        for step, (batch_x, batch_y) in enumerate(loader):          # for each training step
            # train your data...
            print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',
                  batch_x.numpy(), '| batch y: ', batch_y.numpy())
                  
